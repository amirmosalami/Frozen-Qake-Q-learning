{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, rows, columns):\n",
    "        self.map_rows = rows\n",
    "        self.map_cols = columns\n",
    "        \n",
    "    def setGoalState(self, x, y):\n",
    "        self.goal_state = (x, y)\n",
    "        \n",
    "    def setStart(self, x, y):\n",
    "        self.start = (x, y)\n",
    "        \n",
    "    def setHoles(self, holes):\n",
    "        self.holes = holes\n",
    "        \n",
    "    def getGoalState(self):\n",
    "        return self.goal_state\n",
    "    \n",
    "    def getStart(self):\n",
    "        return self.start\n",
    "    \n",
    "    def getHoles(self):\n",
    "        return self.holes\n",
    "        \n",
    "    def getSize(self):\n",
    "        return self.map_rows, self.map_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, x, y):\n",
    "        self.loc = (x, y)\n",
    "        self.isEnd = False\n",
    "        \n",
    "    def getLoc(self):\n",
    "        return self.loc\n",
    "        \n",
    "    def getReward(self):\n",
    "        if self.loc == goal_state:\n",
    "            return 10 \n",
    "        elif self.loc in holes:\n",
    "            return -5\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def isEndFunc(self):\n",
    "        if (self.loc == goal_state):\n",
    "            self.isEnd = True    \n",
    "    \n",
    "    def nextLoc(self, action):        \n",
    "        if action == \"up\":                \n",
    "            nextState = (self.loc[0] - 1, self.loc[1])                \n",
    "        elif action == \"down\":\n",
    "            nextState = (self.loc[0] + 1, self.loc[1])\n",
    "        elif action == \"left\":\n",
    "            nextState = (self.loc[0], self.loc[1] - 1)\n",
    "        else:\n",
    "            nextState = (self.loc[0], self.loc[1] + 1)\n",
    "           \n",
    "        if (nextState[0] >= 0) and (nextState[0] <= map_rows-1):\n",
    "            if (nextState[1] >= 0) and (nextState[1] <= map_cols-1):                    \n",
    "                    return nextState\n",
    "        return self.loc\n",
    "    \n",
    "    def conversion(self):\n",
    "        return map_cols * self.loc[0] + self.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        \n",
    "        #EVIRONMENT\n",
    "        self.env = Environment(map_rows, map_cols)\n",
    "        self.env.setGoalState(goal_state[0], goal_state[1])\n",
    "        self.env.setStart(start[0], start[1])\n",
    "        self.env.setHoles(holes)\n",
    "        \n",
    "        #Q_TABLE\n",
    "        self.state_size = map_rows * map_cols\n",
    "        self.action_size = len(self.actions)\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "        \n",
    "        self.rewards = []\n",
    "                   \n",
    "    def printTable(self):\n",
    "        print(\"------------------- Q LEARNING TABLE ------------------\")\n",
    "        print(self.qtable)\n",
    "        print(\"-------------------------------------------------------\")\n",
    "        \n",
    "    def printPath(self):\n",
    "        rows, cols = self.env.getSize()\n",
    "        \n",
    "        start = self.env.getStart()\n",
    "        state = State(start[0], start[1])      \n",
    "        while True:\n",
    "            print(state.getLoc())\n",
    "            location = state.getLoc()\n",
    "            if state.getLoc()[0]==self.env.getGoalState()[0] and state.getLoc()[1]==self.env.getGoalState()[1]:\n",
    "                break\n",
    "            old_state = state.conversion()\n",
    "            action = np.argmax(self.qtable[old_state, :])                           \n",
    "            nextstate = state.nextLoc(self.actions[action])                   \n",
    "            state = State(nextstate[0],nextstate[1])                                \n",
    "        \n",
    "                 \n",
    "    def q_learning(self):\n",
    "        start = self.env.getStart()\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            state = State(start[0],start[1])\n",
    "            total_rewards = 0\n",
    "            for step in range(max_steps):\n",
    "                exp_exp_tradeoff = random.uniform(0, 1)\n",
    "                old_state  = state.conversion()\n",
    "        \n",
    "                epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "        \n",
    "                if exp_exp_tradeoff > epsilon:\n",
    "                    action = np.argmax(self.qtable[old_state, :])\n",
    "                else:\n",
    "                    action = random.randint(0, len(self.actions)-1)\n",
    "                           \n",
    "                nextState = state.nextLoc(self.actions[action])\n",
    "                new_state  = State(nextState[0], nextState[1]).conversion()\n",
    "                reward = state.getReward()\n",
    "                total_rewards += reward\n",
    "                \n",
    "                self.qtable[old_state, action] = (1 - learning_rate) * self.qtable[old_state, action] + learning_rate * (reward + gamma * np.max(self.qtable[new_state, :]))\n",
    "                \n",
    "                state = State(nextState[0],nextState[1])\n",
    "                \n",
    "            self.rewards.append(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 10000\n",
    "learning_rate = 0.5\n",
    "max_steps = 99\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_rows = 5\n",
    "map_cols = 5\n",
    "goal_state = (4, 4)\n",
    "start = (0, 0)\n",
    "holes = [(0,2), (1,1), (2,3), (3,0), (4,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Q LEARNING TABLE ------------------\n",
      "[[118.65487014 125.95249488 118.65487014 122.34249488]\n",
      " [122.34084073 129.83420514 118.64883458 129.82329207]\n",
      " [123.47691437 136.92457548 117.70826229 137.72000354]\n",
      " [141.67894245 150.22597911 129.7576014  150.23180528]\n",
      " [150.22989997 159.191375   141.71993542 150.23161416]\n",
      " [118.65487014 133.63420514 125.95249488 129.83420514]\n",
      " [118.34248157 137.72021594 121.95249151 137.72021417]\n",
      " [129.72781694 150.23180625 129.8207722  150.21885354]\n",
      " [141.71925398 155.39078603 141.68476474 159.191375  ]\n",
      " [150.23180608 168.6225     150.23180389 159.19137426]\n",
      " [125.95249488 137.92021594 133.63420514 141.72021594]\n",
      " [129.83420514 150.23180625 133.63420514 150.23180625]\n",
      " [141.72021587 159.191375   141.72021589 155.39137346]\n",
      " [146.2318061  164.6225     146.23180625 164.62249999]\n",
      " [159.191375   178.55       155.391375   168.6225    ]\n",
      " [129.63419303 142.97376732 133.92018289 146.23180625]\n",
      " [141.72021594 155.761875   137.92021594 159.191375  ]\n",
      " [150.23180625 165.0125     150.23180625 168.6225    ]\n",
      " [155.391375   174.75       159.191375   178.55      ]\n",
      " [168.6225     189.         168.6225     178.55      ]\n",
      " [137.91397995 146.96819395 146.97360827 155.761875  ]\n",
      " [150.23180624 155.76187488 146.97378107 165.0125    ]\n",
      " [159.191375   165.0125     155.761875   174.75      ]\n",
      " [164.6225     170.75       161.0125     185.        ]\n",
      " [189.55       200.         185.75       200.        ]]\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ag = Agent()\n",
    "ag.q_learning()\n",
    "ag.printTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(1, 0)\n",
      "(2, 0)\n",
      "(2, 1)\n",
      "(3, 1)\n",
      "(3, 2)\n",
      "(3, 3)\n",
      "(3, 4)\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "ag.printPath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
